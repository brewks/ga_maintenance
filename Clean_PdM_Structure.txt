##### 1. Synthetic Data Generator — (generate_degrading_sensor_data.py)

import sqlite3 
import numpy as np
from datetime import datetime, timedelta
import sys

def generate_degrading_sensor_data(db_path, top_params, num_components=10, num_records=1000):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    base_time = datetime.now() - timedelta(hours=num_records)

    for comp_id in range(1, num_components + 1):
        tail_number = f"N{np.random.randint(10000, 99999)}"
        degradation_rate = np.random.uniform(0.5, 1.5)  # Different degradation speed for each component
        initial_factor = np.random.uniform(0.8, 1.2)    # Different starting health level

        for i in range(num_records):
            timestamp = base_time + timedelta(minutes=i)
            for param in top_params:
                # Compute base degradation
                base_val = max((num_records - i * degradation_rate) / num_records, 0)
                
                # Parameter-specific variability
                if param in ['rpm', 'bus_voltage']:
                    noise = np.random.normal(0, 0.02)  # Stable parameters
                elif param in ['oil_press', 'hyd_press']:
                    noise = np.random.normal(0, 0.05) + (np.random.rand() < 0.01) * -0.5  # Occasionally large drops
                else:
                    noise = np.random.normal(0, 0.05)  # Default noise

                # Compute final value
                value = max(base_val * initial_factor + noise, 0) * 100

                # Assign units
                if param in ['oil_press', 'hyd_press', 'brake_press', 'manifold_press']:
                    unit = 'psi'
                elif param in ['cht', 'oil_temp']:
                    unit = '°C'
                elif param == 'rpm':
                    unit = 'rpm'
                elif param == 'bus_voltage':
                    unit = 'volts'
                elif param == 'alternator_current':
                    unit = 'amps'
                else:
                    unit = 'psi'  # safe default

                # Insert into DB
                cursor.execute("""
                    INSERT INTO sensor_data (tail_number, component_id, parameter, value, unit, timestamp, sensor_health)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    tail_number, comp_id, param, value, unit,
                    timestamp.strftime('%Y-%m-%d %H:%M:%S'), 0
                ))

    conn.commit()
    conn.close()
    print(f"✅ Inserted synthetic sensor records successfully.")

# === CALLER LOGIC ===
if __name__ == "__main__":
    # Optional: adjust sys.path if needed
    # sys.path.append("C:\\Users\\workd\\Desktop\\ga_maintenance\\PdM")

    db_path = "C:\\Users\\workd\\Desktop\\ga_maintenance\\PdM\\ga_maintenance.db"
    top_params = ['cht', 'fuel_flow', 'rpm', 'manifold_press', 'bus_voltage',
                  'alternator_current', 'hyd_press', 'brake_press', 'oil_press', 'oil_temp']

    generate_degrading_sensor_data(db_path, top_params)

====================================================================================================
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def plot_sample_variability(db_path, param='oil_press'):
    conn = sqlite3.connect(db_path)
    df = pd.read_sql("SELECT * FROM sensor_data WHERE parameter = ?", conn, params=(param,))
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    conn.close()

    plt.figure(figsize=(12,6))
    sns.lineplot(data=df, x='timestamp', y='value', hue='component_id')
    plt.title(f"{param} variability across components")
    plt.show()

# Example:
# plot_sample_variability("C:/Users/workd/Desktop/ga_maintenance/PdM/ga_maintenance.db", 'oil_press')
=======================================================================================================

##### 2. Data Preparation + Model Training — train_models.py

import sqlite3
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.optimizers import Adam
from datetime import datetime
import joblib
import os
import matplotlib.pyplot as plt

DB_PATH = "C:/Users/workd/Desktop/ga_maintenance/PdM/ga_maintenance.db"
TOP_PARAMS = ['cht', 'fuel_flow', 'rpm', 'manifold_press',
              'bus_voltage', 'alternator_current', 'hyd_press',
              'brake_press', 'oil_press', 'oil_temp']

MIN_SEQUENCE_LENGTH = 10

# Load data
conn = sqlite3.connect(DB_PATH)
sensor_data = pd.read_sql_query("SELECT * FROM sensor_data", conn)
components = pd.read_sql_query("SELECT component_id, remaining_useful_life FROM components", conn)
conn.close()

# Clean data
sensor_data['timestamp'] = pd.to_datetime(sensor_data['timestamp'], errors='coerce')
sensor_data = sensor_data.dropna(subset=['timestamp'])
sensor_data = sensor_data[sensor_data['parameter'].isin(TOP_PARAMS)]

# Pivot
pivoted = sensor_data.pivot_table(index=['component_id', 'timestamp'], columns='parameter', values='value').sort_index().reset_index()
pivoted.ffill(inplace=True)
pivoted.bfill(inplace=True)
pivoted = pivoted.dropna(subset=TOP_PARAMS)

# Normalize
scaler = MinMaxScaler()
pivoted[TOP_PARAMS] = scaler.fit_transform(pivoted[TOP_PARAMS])

# Ensure y_max is valid
y_max = components['remaining_useful_life'].dropna().max()
if pd.isna(y_max) or y_max == 0:
    y_max = 1

# Initialize sequence containers
x_seq = []
y_seq = []
comp_ids = []

# Build sequences
for cid in pivoted['component_id'].unique():
    comp_data = pivoted[pivoted['component_id'] == cid].sort_values('timestamp')
    rul_val = components.loc[components['component_id'] == cid, 'remaining_useful_life'].values
    if rul_val.size == 0 or len(comp_data) < MIN_SEQUENCE_LENGTH or pd.isna(rul_val[0]):
        continue
    for i in range(len(comp_data) - MIN_SEQUENCE_LENGTH + 1):
        x_seq.append(comp_data[TOP_PARAMS].iloc[i:i+MIN_SEQUENCE_LENGTH].values)
        y_seq.append(rul_val[0])
        comp_ids.append(cid)

# Convert and normalize y
x_seq = np.array(x_seq)
y_seq = np.array(y_seq) / y_max

# Remove any NaN sequences
valid = ~np.isnan(x_seq).any(axis=(1,2)) & ~np.isnan(y_seq)
x_seq = x_seq[valid]
y_seq = y_seq[valid]
comp_ids = np.array(comp_ids)[valid]

# Split
x_train, x_test, y_train, y_test, comp_train, comp_test = train_test_split(
    x_seq, y_seq, comp_ids, test_size=0.2, random_state=42
)

# RF inputs (last timestep of sequence)
x_rf_train = x_train[:, -1, :]
x_rf_test = x_test[:, -1, :]

# Remove NaNs from train y
nan_y_mask = ~np.isnan(y_train)
x_rf_train = x_rf_train[nan_y_mask]
y_train_clean = y_train[nan_y_mask]

# Fit RF
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(x_rf_train, y_train_clean)

# Predict
y_rf_pred = rf.predict(x_rf_test) * y_max

print("RF MAE:", mean_absolute_error(y_test * y_max, y_rf_pred))
rmse = root_mean_squared_error(y_test * y_max, y_rf_pred)
print("RF RMSE:", rmse)
print("RF R2:", r2_score(y_test * y_max, y_rf_pred))

os.makedirs("models", exist_ok=True)
joblib.dump(rf, "models/rf_model_rul.pkl")

# LSTM model
model = Sequential([
    Input(shape=(MIN_SEQUENCE_LENGTH, len(TOP_PARAMS))),
    LSTM(64),
    Dense(1)
])
model.compile(optimizer=Adam(0.001), loss='mse')
model.fit(x_train, y_train, epochs=20, batch_size=16, validation_data=(x_test, y_test))
model.save("models/model_lstm_rul.keras")

# LSTM predictions
y_lstm_pred = model.predict(x_test).flatten() * y_max

# Plot predictions
plt.figure(figsize=(10, 5))
plt.scatter(y_test * y_max, y_rf_pred, alpha=0.6, label="RF predictions")
plt.scatter(y_test * y_max, y_lstm_pred, alpha=0.6, label="LSTM predictions")
plt.plot([0, y_max], [0, y_max], 'k--', label="Ideal")
plt.xlabel("Actual RUL")
plt.ylabel("Predicted RUL")
plt.title("Predicted vs Actual RUL")
plt.legend()
plt.grid(True)
plt.show()

# Residuals plot
rf_residuals = (y_test * y_max) - y_rf_pred
lstm_residuals = (y_test * y_max) - y_lstm_pred

plt.figure(figsize=(10, 5))
plt.hist(rf_residuals, bins=50, alpha=0.5, label="RF residuals")
plt.hist(lstm_residuals, bins=50, alpha=0.5, label="LSTM residuals")
plt.xlabel("Residual (Actual - Predicted RUL)")
plt.ylabel("Frequency")
plt.title("Residual Distribution")
plt.legend()
plt.grid(True)
plt.show()

# Log predictions
now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
model_id = 99

conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()
cursor.execute("DELETE FROM component_predictions WHERE model_id = ?", (model_id,))
records = [
    (int(cid), model_id, 'remaining_life', float(pred), 0.85, '100h', 'Predicted via LSTM', now)
    for cid, pred in zip(comp_test, y_lstm_pred)
]
cursor.executemany("""
    INSERT INTO component_predictions (
        component_id, model_id, prediction_type, predicted_value,
        confidence, time_horizon, explanation, prediction_time
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
""", records)
conn.commit()
conn.close()







====================================================================================================
##### 3. Dashboard - app.py
import streamlit as st
import pandas as pd
import sqlite3
import matplotlib.pyplot as plt
import seaborn as sns
import time

# === CONFIG ===
DB_PATH = "C:/Users/workd/Desktop/ga_maintenance/PdM/ga_maintenance.db"

# === FUNCTIONS ===
def load_data(query):
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df

def plot_rul_bar(df):
    fig, ax = plt.subplots(figsize=(10, 5))
    sns.barplot(data=df, x=df['component_id'].astype(str), y='predicted_value', ax=ax, palette='Blues')
    ax.set_xlabel("Component ID")
    ax.set_ylabel("Predicted RUL (hours)")
    ax.set_title("Latest Component RUL Predictions")
    plt.xticks(rotation=45)
    st.pyplot(fig)

def plot_confidence_rul(df):
    fig, ax = plt.subplots(figsize=(10, 5))
    sns.scatterplot(data=df, x='component_id', y='predicted_value', size='confidence', hue='confidence', palette='coolwarm', ax=ax, sizes=(50, 300))
    ax.set_title("RUL vs Component ID (Size = Confidence)")
    plt.xticks(rotation=45)
    st.pyplot(fig)

def plot_rul_trend(df):
    if 'prediction_time' in df.columns:
        df['prediction_time'] = pd.to_datetime(df['prediction_time'], errors='coerce')
        fig, ax = plt.subplots(figsize=(10, 5))
        sns.lineplot(data=df, x='prediction_time', y='predicted_value', hue='component_id', marker="o", ax=ax)
        ax.set_title("RUL Predictions Over Time")
        plt.xticks(rotation=45)
        st.pyplot(fig)

# === APP LAYOUT ===
st.set_page_config(page_title="GA PdM Dashboard", layout="wide")
st.markdown("""
<style>
    .stApp { background-color: #f2f6fc; }
    .big-font { font-size:20px !important; color: #003366; }
</style>
""", unsafe_allow_html=True)

st.title("✈️ General Aviation Predictive Maintenance Dashboard")
st.markdown('<p class="big-font">Live aircraft system health, predictive maintenance insights, and alerts</p>', unsafe_allow_html=True)

# Auto-refresh interval
refresh_interval = st.sidebar.slider("Auto-refresh (seconds)", 0, 60, 10)

view_choice = st.sidebar.radio(
    "Select View",
    ["Components Needing Attention", "Dashboard Snapshot", "Latest Predictions"]
)

# === LOAD DATA ===
if view_choice == "Components Needing Attention":
    df = load_data("SELECT * FROM components_needing_attention;")
elif view_choice == "Dashboard Snapshot":
    df = load_data("SELECT * FROM dashboard_snapshot_view;")
else:
    df = load_data("SELECT * FROM component_predictions ORDER BY prediction_time DESC LIMIT 100;")

# === DISPLAY DATA ===
st.subheader(view_choice)
st.write(f"### Data Summary: {len(df)} records loaded")
st.dataframe(df)

if df.empty:
    st.warning("⚠ No data available for this view.")
else:
    if view_choice == "Latest Predictions":
        st.write("### Predicted Remaining Useful Life (RUL)")
        plot_rul_bar(df)

        st.write("### RUL vs Component ID with Confidence")
        plot_confidence_rul(df)

        st.write("### RUL Prediction Trends")
        plot_rul_trend(df)

    if "confidence" in df.columns and "prediction_type" in df.columns:
        critical_alerts = df[(df["confidence"] > 0.9) & (df["prediction_type"] == "failure")]
        if not critical_alerts.empty:
            st.error(f"🚨 {len(critical_alerts)} CRITICAL failure predictions detected!")

    if "confidence" in df.columns:
        conf_level = st.slider("Minimum Confidence", 0.0, 1.0, 0.7)
        filtered_df = df[df['confidence'] >= conf_level]
        st.write(f"### Filtered Predictions (Confidence >= {conf_level})")
        st.dataframe(filtered_df)

        if not filtered_df.empty:
            st.download_button(
                "Download Filtered Data",
                filtered_df.to_csv(index=False).encode(),
                file_name="filtered_predictions.csv",
                mime="text/csv"
            )

# === AUTO-REFRESH ===
if refresh_interval > 0:
    st.info(f"⏳ Auto-refreshing every {refresh_interval} seconds...")
    time.sleep(refresh_interval)
    st.experimental_rerun()
